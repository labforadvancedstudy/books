# L3-01: ì¹´ë©”ë¼ì™€ Vision í”„ë ˆì„ì›Œí¬
## ì‹¤ì‹œê°„ ì¬ë£Œ ì¸ì‹, AR ìš”ë¦¬ ê°€ì´ë“œ, ì˜ì–‘ ë¶„ì„

---

> **"The camera is becoming the keyboard of the future."**

ì¹´ë©”ë¼ì™€ AIë¥¼ ê²°í•©í•´ ìš”ë¦¬ë¥¼ í˜ì‹ ì ìœ¼ë¡œ ë³€í™”ì‹œí‚µë‹ˆë‹¤.

---

## ğŸ¯ ëª©í‘œ

**ì™„ì„± í›„ ê²°ê³¼ë¬¼**:
- ì‹¤ì‹œê°„ ì¬ë£Œ ìŠ¤ìº”
- AR ë ˆì‹œí”¼ ì˜¤ë²„ë ˆì´  
- ìŒì‹ ë³¼ë¥¨ ì¸¡ì •
- ì¹¼ë¡œë¦¬ ì¶”ì •
- ë°”ì½”ë“œ/QR ìŠ¤ìº”

---

## ğŸ“¸ ì¹´ë©”ë¼ UI êµ¬í˜„

### 1ë‹¨ê³„: ìŠ¤ìº” ì¹´ë©”ë¼ ë·°

**Features/Camera/ScannerCameraView.swift**:

```swift
import SwiftUI
import AVFoundation
import Vision
import ARKit

struct ScannerCameraView: View {
    @StateObject private var camera = CameraViewModel()
    @StateObject private var recognizer = IngredientRecognizer()
    @State private var showingResults = false
    @State private var capturedImage: UIImage?
    @State private var scanMode: ScanMode = .ingredient
    @State private var showingARMode = false
    
    enum ScanMode: String, CaseIterable {
        case ingredient = "ì¬ë£Œ"
        case barcode = "ë°”ì½”ë“œ"
        case nutrition = "ì˜ì–‘"
        case volume = "ìš©ëŸ‰"
        
        var icon: String {
            switch self {
            case .ingredient: return "leaf"
            case .barcode: return "barcode"
            case .nutrition: return "chart.pie"
            case .volume: return "cube"
            }
        }
    }
    
    var body: some View {
        ZStack {
            // ì¹´ë©”ë¼ í”„ë¦¬ë·°
            CameraPreviewView(session: camera.session)
                .ignoresSafeArea()
                .overlay(
                    ScanningOverlay(
                        detectedObjects: camera.detectedObjects,
                        scanMode: scanMode
                    )
                )
                .gesture(
                    // íƒ­ìœ¼ë¡œ í¬ì»¤ìŠ¤
                    DragGesture(minimumDistance: 0)
                        .onEnded { value in
                            camera.focus(at: value.location)
                        }
                )
                .gesture(
                    // í•€ì¹˜ë¡œ ì¤Œ
                    MagnificationGesture()
                        .onChanged { value in
                            camera.zoom(scale: value)
                        }
                )
            
            // UI ì˜¤ë²„ë ˆì´
            VStack {
                // ìƒë‹¨ ì»¨íŠ¸ë¡¤
                HStack {
                    Button {
                        camera.toggleFlash()
                    } label: {
                        Image(systemName: camera.isFlashOn ? "bolt.fill" : "bolt.slash")
                            .font(.title2)
                            .foregroundColor(.white)
                            .frame(width: 44, height: 44)
                            .background(Color.black.opacity(0.5))
                            .clipShape(Circle())
                    }
                    
                    Spacer()
                    
                    // ëª¨ë“œ ì„ íƒ
                    Picker("ìŠ¤ìº” ëª¨ë“œ", selection: $scanMode) {
                        ForEach(ScanMode.allCases, id: \.self) { mode in
                            Label(mode.rawValue, systemImage: mode.icon)
                                .tag(mode)
                        }
                    }
                    .pickerStyle(SegmentedPickerStyle())
                    .frame(width: 200)
                    .background(Color.black.opacity(0.5))
                    .cornerRadius(8)
                    
                    Spacer()
                    
                    Button {
                        showingARMode.toggle()
                    } label: {
                        Image(systemName: "arkit")
                            .font(.title2)
                            .foregroundColor(.white)
                            .frame(width: 44, height: 44)
                            .background(Color.black.opacity(0.5))
                            .clipShape(Circle())
                    }
                }
                .padding()
                
                Spacer()
                
                // ì‹¤ì‹œê°„ ì¸ì‹ ê²°ê³¼
                if !camera.recognizedItems.isEmpty {
                    RecognitionResultsBar(items: camera.recognizedItems)
                        .padding(.horizontal)
                }
                
                // í•˜ë‹¨ ì»¨íŠ¸ë¡¤
                HStack(spacing: 40) {
                    // ê°¤ëŸ¬ë¦¬
                    Button {
                        camera.openPhotoLibrary()
                    } label: {
                        if let lastPhoto = camera.lastPhoto {
                            Image(uiImage: lastPhoto)
                                .resizable()
                                .scaledToFill()
                                .frame(width: 50, height: 50)
                                .clipShape(RoundedRectangle(cornerRadius: 8))
                        } else {
                            Image(systemName: "photo")
                                .font(.title2)
                                .foregroundColor(.white)
                                .frame(width: 50, height: 50)
                                .background(Color.black.opacity(0.5))
                                .clipShape(RoundedRectangle(cornerRadius: 8))
                        }
                    }
                    
                    // ìº¡ì²˜ ë²„íŠ¼
                    Button {
                        capturePhoto()
                    } label: {
                        ZStack {
                            Circle()
                                .fill(Color.white)
                                .frame(width: 70, height: 70)
                            
                            Circle()
                                .stroke(Color.white, lineWidth: 3)
                                .frame(width: 80, height: 80)
                        }
                    }
                    
                    // ì¹´ë©”ë¼ ì „í™˜
                    Button {
                        camera.switchCamera()
                    } label: {
                        Image(systemName: "camera.rotate")
                            .font(.title2)
                            .foregroundColor(.white)
                            .frame(width: 50, height: 50)
                            .background(Color.black.opacity(0.5))
                            .clipShape(RoundedRectangle(cornerRadius: 8))
                    }
                }
                .padding(.bottom, 30)
            }
        }
        .sheet(isPresented: $showingResults) {
            if let image = capturedImage {
                ScanResultsView(
                    image: image,
                    scanMode: scanMode,
                    recognizedItems: camera.recognizedItems
                )
            }
        }
        .fullScreenCover(isPresented: $showingARMode) {
            ARCookingGuideView()
        }
        .onAppear {
            camera.startSession()
        }
        .onDisappear {
            camera.stopSession()
        }
    }
    
    private func capturePhoto() {
        camera.capturePhoto { image in
            self.capturedImage = image
            
            Task {
                // ëª¨ë“œë³„ ì²˜ë¦¬
                switch scanMode {
                case .ingredient:
                    await processIngredientScan(image)
                case .barcode:
                    await processBarcodeaScan(image)
                case .nutrition:
                    await processNutritionScan(image)
                case .volume:
                    await processVolumeScan(image)
                }
                
                showingResults = true
            }
        }
        
        // í–…í‹± í”¼ë“œë°±
        let impactFeedback = UIImpactFeedbackGenerator(style: .medium)
        impactFeedback.impactOccurred()
    }
    
    private func processIngredientScan(_ image: UIImage) async {
        do {
            let ingredients = try await recognizer.recognizeIngredients(from: image)
            await MainActor.run {
                camera.recognizedItems = ingredients.map { RecognizedItem(from: $0) }
            }
        } catch {
            print("ì¬ë£Œ ì¸ì‹ ì‹¤íŒ¨: \(error)")
        }
    }
}

// MARK: - Camera Preview

struct CameraPreviewView: UIViewRepresentable {
    let session: AVCaptureSession
    
    func makeUIView(context: Context) -> UIView {
        let view = UIView()
        
        let previewLayer = AVCaptureVideoPreviewLayer(session: session)
        previewLayer.videoGravity = .resizeAspectFill
        view.layer.addSublayer(previewLayer)
        
        DispatchQueue.main.async {
            previewLayer.frame = view.bounds
        }
        
        return view
    }
    
    func updateUIView(_ uiView: UIView, context: Context) {
        if let previewLayer = uiView.layer.sublayers?.first as? AVCaptureVideoPreviewLayer {
            DispatchQueue.main.async {
                previewLayer.frame = uiView.bounds
            }
        }
    }
}

// MARK: - Scanning Overlay

struct ScanningOverlay: View {
    let detectedObjects: [DetectedObject]
    let scanMode: ScannerCameraView.ScanMode
    @State private var animationPhase = 0.0
    
    var body: some View {
        ZStack {
            // ìŠ¤ìº” ê°€ì´ë“œ
            if scanMode == .ingredient || scanMode == .nutrition {
                RoundedRectangle(cornerRadius: 20)
                    .stroke(
                        LinearGradient(
                            colors: [.blue, .purple],
                            startPoint: .topLeading,
                            endPoint: .bottomTrailing
                        ),
                        lineWidth: 3
                    )
                    .frame(width: 300, height: 300)
                    .opacity(0.8)
                    .scaleEffect(1.0 + sin(animationPhase) * 0.05)
                    .animation(
                        Animation.easeInOut(duration: 2)
                            .repeatForever(autoreverses: true),
                        value: animationPhase
                    )
            }
            
            // ê°ì§€ëœ ê°ì²´ í‘œì‹œ
            ForEach(detectedObjects) { object in
                ObjectBoundingBox(object: object)
            }
            
            // ìŠ¤ìº” ëª¨ë“œ í‘œì‹œ
            VStack {
                Spacer()
                
                HStack {
                    Image(systemName: scanMode.icon)
                    Text("\(scanMode.rawValue) ìŠ¤ìº” ì¤‘...")
                }
                .padding()
                .background(Color.black.opacity(0.7))
                .foregroundColor(.white)
                .cornerRadius(20)
                .padding(.bottom, 100)
            }
        }
        .onAppear {
            animationPhase = .pi * 2
        }
    }
}

struct ObjectBoundingBox: View {
    let object: DetectedObject
    
    var body: some View {
        GeometryReader { geometry in
            let rect = VNImageRectForNormalizedRect(
                object.boundingBox,
                Int(geometry.size.width),
                Int(geometry.size.height)
            )
            
            ZStack(alignment: .topLeading) {
                Rectangle()
                    .stroke(object.color, lineWidth: 2)
                    .frame(width: rect.width, height: rect.height)
                    .position(
                        x: rect.midX,
                        y: geometry.size.height - rect.midY
                    )
                
                // ë¼ë²¨
                Text(object.label)
                    .font(.caption)
                    .padding(4)
                    .background(object.color)
                    .foregroundColor(.white)
                    .cornerRadius(4)
                    .position(
                        x: rect.minX,
                        y: geometry.size.height - rect.maxY
                    )
                
                // ì‹ ë¢°ë„
                if object.confidence > 0 {
                    Text("\(Int(object.confidence * 100))%")
                        .font(.caption2)
                        .padding(2)
                        .background(Color.black.opacity(0.7))
                        .foregroundColor(.white)
                        .cornerRadius(4)
                        .position(
                            x: rect.maxX,
                            y: geometry.size.height - rect.maxY
                        )
                }
            }
        }
    }
}
```

### 2ë‹¨ê³„: ì¹´ë©”ë¼ ë·°ëª¨ë¸

**Features/Camera/CameraViewModel.swift**:

```swift
import AVFoundation
import Vision
import Photos
import Combine
import UIKit

@MainActor
class CameraViewModel: NSObject, ObservableObject {
    @Published var session = AVCaptureSession()
    @Published var isFlashOn = false
    @Published var zoomFactor: CGFloat = 1.0
    @Published var recognizedItems: [RecognizedItem] = []
    @Published var detectedObjects: [DetectedObject] = []
    @Published var lastPhoto: UIImage?
    @Published var isProcessing = false
    
    private var photoOutput = AVCapturePhotoOutput()
    private var videoOutput = AVCaptureVideoDataOutput()
    private var currentDevice: AVCaptureDevice?
    private var captureCompletionHandler: ((UIImage) -> Void)?
    
    // Vision
    private var requests = [VNRequest]()
    private let visionQueue = DispatchQueue(label: "com.app.vision", qos: .userInitiated)
    
    override init() {
        super.init()
        setupCamera()
        setupVision()
        loadLastPhoto()
    }
    
    private func setupCamera() {
        session.beginConfiguration()
        session.sessionPreset = .photo
        
        // ì¹´ë©”ë¼ ì¶”ê°€
        guard let camera = AVCaptureDevice.default(
            .builtInWideAngleCamera,
            for: .video,
            position: .back
        ) else { return }
        
        currentDevice = camera
        
        do {
            let input = try AVCaptureDeviceInput(device: camera)
            if session.canAddInput(input) {
                session.addInput(input)
            }
            
            // ì‚¬ì§„ ì¶œë ¥
            if session.canAddOutput(photoOutput) {
                session.addOutput(photoOutput)
                photoOutput.isHighResolutionCaptureEnabled = true
                photoOutput.maxPhotoQualityPrioritization = .quality
            }
            
            // ë¹„ë””ì˜¤ ì¶œë ¥ (ì‹¤ì‹œê°„ ë¶„ì„ìš©)
            videoOutput.setSampleBufferDelegate(self, queue: visionQueue)
            if session.canAddOutput(videoOutput) {
                session.addOutput(videoOutput)
            }
            
        } catch {
            print("ì¹´ë©”ë¼ ì„¤ì • ì‹¤íŒ¨: \(error)")
        }
        
        session.commitConfiguration()
    }
    
    private func setupVision() {
        // ê°ì²´ ì¸ì‹
        guard let model = try? VNCoreMLModel(for: YOLOv3().model) else { return }
        
        let objectDetection = VNCoreMLRequest(model: model) { [weak self] request, error in
            self?.processDetections(request.results)
        }
        objectDetection.imageCropAndScaleOption = .scaleFill
        
        // ë°”ì½”ë“œ ì¸ì‹
        let barcodeRequest = VNDetectBarcodesRequest { [weak self] request, error in
            self?.processBarcodes(request.results)
        }
        
        // í…ìŠ¤íŠ¸ ì¸ì‹
        let textRequest = VNRecognizeTextRequest { [weak self] request, error in
            self?.processText(request.results)
        }
        textRequest.recognitionLevel = .accurate
        textRequest.recognitionLanguages = ["ko-KR", "en-US"]
        textRequest.usesLanguageCorrection = true
        
        requests = [objectDetection, barcodeRequest, textRequest]
    }
    
    func startSession() {
        if !session.isRunning {
            DispatchQueue.global(qos: .background).async { [weak self] in
                self?.session.startRunning()
            }
        }
    }
    
    func stopSession() {
        if session.isRunning {
            DispatchQueue.global(qos: .background).async { [weak self] in
                self?.session.stopRunning()
            }
        }
    }
    
    func capturePhoto(completion: @escaping (UIImage) -> Void) {
        captureCompletionHandler = completion
        
        let settings = AVCapturePhotoSettings()
        settings.flashMode = isFlashOn ? .on : .off
        
        // HEIF í˜•ì‹ ì‚¬ìš© (ë” ë‚˜ì€ ì••ì¶•)
        if photoOutput.availablePhotoCodecTypes.contains(.hevc) {
            settings.photoCodecType = .hevc
        }
        
        // ê¹Šì´ ë°ì´í„° ìº¡ì²˜
        if photoOutput.isDepthDataDeliverySupported {
            settings.isDepthDataDeliveryEnabled = true
        }
        
        // í¬íŠ¸ë ˆì´íŠ¸ íš¨ê³¼
        if photoOutput.isPortraitEffectsMatteDeliverySupported {
            settings.isPortraitEffectsMatteDeliveryEnabled = true
        }
        
        photoOutput.capturePhoto(with: settings, delegate: self)
    }
    
    func toggleFlash() {
        isFlashOn.toggle()
    }
    
    func switchCamera() {
        session.beginConfiguration()
        
        // í˜„ì¬ ì…ë ¥ ì œê±°
        session.inputs.forEach { session.removeInput($0) }
        
        // ìƒˆ ì¹´ë©”ë¼ ì„ íƒ
        let position: AVCaptureDevice.Position = currentDevice?.position == .back ? .front : .back
        
        guard let newCamera = AVCaptureDevice.default(
            .builtInWideAngleCamera,
            for: .video,
            position: position
        ) else { return }
        
        do {
            let input = try AVCaptureDeviceInput(device: newCamera)
            if session.canAddInput(input) {
                session.addInput(input)
                currentDevice = newCamera
            }
        } catch {
            print("ì¹´ë©”ë¼ ì „í™˜ ì‹¤íŒ¨: \(error)")
        }
        
        session.commitConfiguration()
    }
    
    func zoom(scale: CGFloat) {
        guard let device = currentDevice else { return }
        
        do {
            try device.lockForConfiguration()
            
            let maxZoom = min(device.activeFormat.videoMaxZoomFactor, 10.0)
            let newZoom = max(1.0, min(scale * zoomFactor, maxZoom))
            
            device.videoZoomFactor = newZoom
            zoomFactor = newZoom
            
            device.unlockForConfiguration()
        } catch {
            print("ì¤Œ ì„¤ì • ì‹¤íŒ¨: \(error)")
        }
    }
    
    func focus(at point: CGPoint) {
        guard let device = currentDevice else { return }
        
        do {
            try device.lockForConfiguration()
            
            if device.isFocusPointOfInterestSupported {
                device.focusPointOfInterest = point
                device.focusMode = .autoFocus
            }
            
            if device.isExposurePointOfInterestSupported {
                device.exposurePointOfInterest = point
                device.exposureMode = .autoExpose
            }
            
            device.unlockForConfiguration()
            
            // í¬ì»¤ìŠ¤ ì• ë‹ˆë©”ì´ì…˜ í”¼ë“œë°±
            let generator = UIImpactFeedbackGenerator(style: .light)
            generator.impactOccurred()
        } catch {
            print("í¬ì»¤ìŠ¤ ì„¤ì • ì‹¤íŒ¨: \(error)")
        }
    }
    
    func openPhotoLibrary() {
        // Photo picker ì—´ê¸°
        // SwiftUIì˜ PhotosPicker ì‚¬ìš©
    }
    
    private func loadLastPhoto() {
        let fetchOptions = PHFetchOptions()
        fetchOptions.sortDescriptors = [NSSortDescriptor(key: "creationDate", ascending: false)]
        fetchOptions.fetchLimit = 1
        
        let fetchResult = PHAsset.fetchAssets(with: .image, options: fetchOptions)
        
        guard let asset = fetchResult.firstObject else { return }
        
        let manager = PHImageManager.default()
        let options = PHImageRequestOptions()
        options.isSynchronous = false
        options.deliveryMode = .highQualityFormat
        
        manager.requestImage(
            for: asset,
            targetSize: CGSize(width: 100, height: 100),
            contentMode: .aspectFill,
            options: options
        ) { [weak self] image, _ in
            DispatchQueue.main.async {
                self?.lastPhoto = image
            }
        }
    }
}

// MARK: - AVCapturePhotoCaptureDelegate

extension CameraViewModel: AVCapturePhotoCaptureDelegate {
    func photoOutput(_ output: AVCapturePhotoOutput, didFinishProcessingPhoto photo: AVCapturePhoto, error: Error?) {
        guard error == nil,
              let imageData = photo.fileDataRepresentation(),
              let image = UIImage(data: imageData) else { return }
        
        // ì´ë¯¸ì§€ ì €ì¥
        UIImageWriteToSavedPhotosAlbum(image, nil, nil, nil)
        
        DispatchQueue.main.async {
            self.lastPhoto = image
            self.captureCompletionHandler?(image)
            self.captureCompletionHandler = nil
        }
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate

extension CameraViewModel: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard !isProcessing,
              let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        isProcessing = true
        
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        
        do {
            try handler.perform(requests)
        } catch {
            print("Vision ì²˜ë¦¬ ì‹¤íŒ¨: \(error)")
        }
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            self.isProcessing = false
        }
    }
    
    private func processDetections(_ results: [Any]?) {
        guard let results = results as? [VNRecognizedObjectObservation] else { return }
        
        let objects = results.compactMap { observation -> DetectedObject? in
            guard observation.confidence > 0.5 else { return nil }
            
            return DetectedObject(
                id: UUID(),
                label: observation.labels.first?.identifier ?? "Unknown",
                confidence: observation.confidence,
                boundingBox: observation.boundingBox,
                color: .blue
            )
        }
        
        DispatchQueue.main.async {
            self.detectedObjects = objects
        }
    }
    
    private func processBarcodes(_ results: [Any]?) {
        guard let results = results as? [VNBarcodeObservation] else { return }
        
        for barcode in results {
            if let payload = barcode.payloadStringValue {
                print("ë°”ì½”ë“œ ê°ì§€: \(payload)")
                
                // ë°”ì½”ë“œë¡œ ì œí’ˆ ì¡°íšŒ
                Task {
                    await lookupProduct(barcode: payload)
                }
            }
        }
    }
    
    private func processText(_ results: [Any]?) {
        guard let results = results as? [VNRecognizedTextObservation] else { return }
        
        for observation in results {
            guard let text = observation.topCandidates(1).first?.string else { continue }
            
            // ì˜ì–‘ ì •ë³´ ì¶”ì¶œ
            if text.contains("ì¹¼ë¡œë¦¬") || text.contains("kcal") {
                extractNutritionInfo(from: text)
            }
        }
    }
}

// MARK: - Supporting Types

struct DetectedObject: Identifiable {
    let id: UUID
    let label: String
    let confidence: Float
    let boundingBox: CGRect
    let color: Color
}

struct RecognizedItem: Identifiable {
    let id: UUID
    let name: String
    let category: String
    let confidence: Double
    let nutrition: NutritionInfo?
}
```

### 3ë‹¨ê³„: AR ìš”ë¦¬ ê°€ì´ë“œ

**Features/Camera/ARCookingGuideView.swift**:

```swift
import SwiftUI
import RealityKit
import ARKit
import Combine

struct ARCookingGuideView: View {
    @StateObject private var arViewModel = ARCookingViewModel()
    @State private var currentStep = 0
    @State private var showingStepDetail = false
    @Environment(\.dismiss) private var dismiss
    
    var body: some View {
        ZStack {
            // AR View
            ARViewContainer(viewModel: arViewModel)
                .ignoresSafeArea()
            
            // UI ì˜¤ë²„ë ˆì´
            VStack {
                // í—¤ë”
                HStack {
                    Button {
                        dismiss()
                    } label: {
                        Image(systemName: "xmark.circle.fill")
                            .font(.title)
                            .foregroundColor(.white)
                            .background(Color.black.opacity(0.5))
                            .clipShape(Circle())
                    }
                    
                    Spacer()
                    
                    Text(arViewModel.recipe?.name ?? "AR ìš”ë¦¬ ê°€ì´ë“œ")
                        .font(.headline)
                        .padding(.horizontal, 16)
                        .padding(.vertical, 8)
                        .background(Color.black.opacity(0.7))
                        .foregroundColor(.white)
                        .cornerRadius(20)
                    
                    Spacer()
                    
                    Button {
                        arViewModel.toggleARMode()
                    } label: {
                        Image(systemName: arViewModel.isAREnabled ? "arkit" : "arkit.slash")
                            .font(.title)
                            .foregroundColor(.white)
                            .background(Color.black.opacity(0.5))
                            .clipShape(Circle())
                    }
                }
                .padding()
                
                Spacer()
                
                // í˜„ì¬ ë‹¨ê³„ í‘œì‹œ
                if let step = arViewModel.currentStep {
                    StepOverlay(step: step, onNext: {
                        arViewModel.nextStep()
                    })
                    .padding()
                }
                
                // AR ì¸ë””ì¼€ì´í„°
                if arViewModel.isTracking {
                    HStack {
                        Circle()
                            .fill(Color.green)
                            .frame(width: 10, height: 10)
                        Text("í‘œë©´ ì¸ì‹ë¨")
                            .font(.caption)
                            .foregroundColor(.white)
                    }
                    .padding(8)
                    .background(Color.black.opacity(0.7))
                    .cornerRadius(20)
                    .padding(.bottom)
                }
            }
        }
        .onAppear {
            arViewModel.startAR()
        }
        .onDisappear {
            arViewModel.stopAR()
        }
    }
}

struct ARViewContainer: UIViewRepresentable {
    let viewModel: ARCookingViewModel
    
    func makeUIView(context: Context) -> ARView {
        let arView = ARView(frame: .zero)
        
        // AR ì„¤ì •
        let config = ARWorldTrackingConfiguration()
        config.planeDetection = [.horizontal]
        config.environmentTexturing = .automatic
        
        arView.session.run(config)
        arView.session.delegate = context.coordinator
        
        // ì œìŠ¤ì²˜ ì¶”ê°€
        let tapGesture = UITapGestureRecognizer(
            target: context.coordinator,
            action: #selector(Coordinator.handleTap)
        )
        arView.addGestureRecognizer(tapGesture)
        
        viewModel.arView = arView
        
        return arView
    }
    
    func updateUIView(_ uiView: ARView, context: Context) {
        // ì—…ë°ì´íŠ¸ í•„ìš”ì‹œ ì²˜ë¦¬
    }
    
    func makeCoordinator() -> Coordinator {
        Coordinator(viewModel: viewModel)
    }
    
    class Coordinator: NSObject, ARSessionDelegate {
        let viewModel: ARCookingViewModel
        
        init(viewModel: ARCookingViewModel) {
            self.viewModel = viewModel
        }
        
        @objc func handleTap(gesture: UITapGestureRecognizer) {
            guard let arView = viewModel.arView else { return }
            
            let location = gesture.location(in: arView)
            
            // ë ˆì´ìºìŠ¤íŠ¸ë¡œ í‘œë©´ ì°¾ê¸°
            let results = arView.raycast(
                from: location,
                allowing: .existingPlaneGeometry,
                alignment: .horizontal
            )
            
            if let firstResult = results.first {
                viewModel.placeContent(at: firstResult)
            }
        }
        
        func session(_ session: ARSession, didUpdate frame: ARFrame) {
            // íŠ¸ë˜í‚¹ ìƒíƒœ ì—…ë°ì´íŠ¸
            DispatchQueue.main.async {
                self.viewModel.updateTrackingState(frame.camera.trackingState)
            }
        }
        
        func session(_ session: ARSession, didAdd anchors: [ARAnchor]) {
            for anchor in anchors {
                if let planeAnchor = anchor as? ARPlaneAnchor {
                    viewModel.addPlane(planeAnchor)
                }
            }
        }
    }
}

@MainActor
class ARCookingViewModel: ObservableObject {
    @Published var recipe: Recipe?
    @Published var currentStep: CookingStep?
    @Published var isAREnabled = true
    @Published var isTracking = false
    
    weak var arView: ARView?
    private var anchorEntities: [AnchorEntity] = []
    private var stepEntities: [ModelEntity] = []
    
    func startAR() {
        loadRecipe()
        setupARContent()
    }
    
    func stopAR() {
        arView?.session.pause()
        clearARContent()
    }
    
    func toggleARMode() {
        isAREnabled.toggle()
        
        if isAREnabled {
            showARContent()
        } else {
            hideARContent()
        }
    }
    
    func nextStep() {
        guard let recipe = recipe,
              let currentIndex = recipe.steps.firstIndex(where: { $0.id == currentStep?.id }) else { return }
        
        if currentIndex < recipe.steps.count - 1 {
            currentStep = recipe.steps[currentIndex + 1]
            updateARContent()
        }
    }
    
    func placeContent(at result: ARRaycastResult) {
        guard let arView = arView else { return }
        
        // ì•µì»¤ ìƒì„±
        let anchor = AnchorEntity(world: result.worldTransform)
        arView.scene.addAnchor(anchor)
        
        // 3D ì½˜í…ì¸  ì¶”ê°€
        if let step = currentStep {
            let content = create3DContent(for: step)
            anchor.addChild(content)
            anchorEntities.append(anchor)
        }
        
        // í–…í‹± í”¼ë“œë°±
        let generator = UIImpactFeedbackGenerator(style: .medium)
        generator.impactOccurred()
    }
    
    private func create3DContent(for step: CookingStep) -> ModelEntity {
        // ë‹¨ê³„ë³„ 3D ëª¨ë¸ ìƒì„±
        let mesh: MeshResource
        let material = SimpleMaterial(color: .systemBlue, isMetallic: true)
        
        switch step.type {
        case .cutting:
            // ì¹¼ê³¼ ë„ë§ˆ ëª¨ë¸
            mesh = MeshResource.generateBox(size: [0.2, 0.01, 0.3])
            
        case .mixing:
            // ë³¼ê³¼ ìŠ¤í‘¼ ëª¨ë¸
            mesh = MeshResource.generateSphere(radius: 0.15)
            
        case .cooking:
            // íŒ¬ ëª¨ë¸
            mesh = MeshResource.generateCylinder(height: 0.05, radius: 0.2)
            
        case .plating:
            // ì ‘ì‹œ ëª¨ë¸
            mesh = MeshResource.generatePlane(width: 0.3, depth: 0.3)
            
        default:
            mesh = MeshResource.generateBox(size: 0.1)
        }
        
        let model = ModelEntity(mesh: mesh, materials: [material])
        
        // í…ìŠ¤íŠ¸ ë¼ë²¨ ì¶”ê°€
        if let textMesh = try? MeshResource.generateText(
            step.title,
            extrusionDepth: 0.01,
            font: .systemFont(ofSize: 0.05)
        ) {
            let textEntity = ModelEntity(mesh: textMesh, materials: [material])
            textEntity.position = [0, 0.2, 0]
            model.addChild(textEntity)
        }
        
        // ì• ë‹ˆë©”ì´ì…˜ ì¶”ê°€
        addAnimation(to: model, type: step.animationType)
        
        return model
    }
    
    private func addAnimation(to entity: ModelEntity, type: AnimationType?) {
        guard let type = type else { return }
        
        switch type {
        case .rotate:
            entity.transform.rotation = simd_quatf(angle: 0, axis: [0, 1, 0])
            
            var transform = entity.transform
            transform.rotation = simd_quatf(angle: .pi * 2, axis: [0, 1, 0])
            
            entity.move(
                to: transform,
                relativeTo: entity.parent,
                duration: 3,
                timingFunction: .linear
            )
            
        case .bounce:
            let originalPosition = entity.position
            
            var transform = entity.transform
            transform.translation.y += 0.1
            
            entity.move(
                to: transform,
                relativeTo: entity.parent,
                duration: 1,
                timingFunction: .easeInOut
            )
            
        case .pulse:
            let originalScale = entity.scale
            
            var transform = entity.transform
            transform.scale = originalScale * 1.2
            
            entity.move(
                to: transform,
                relativeTo: entity.parent,
                duration: 0.5,
                timingFunction: .easeInOut
            )
        }
    }
    
    func updateTrackingState(_ state: ARCamera.TrackingState) {
        switch state {
        case .normal:
            isTracking = true
        case .limited:
            isTracking = false
        case .notAvailable:
            isTracking = false
        }
    }
    
    func addPlane(_ anchor: ARPlaneAnchor) {
        // í‰ë©´ ì‹œê°í™” (ì˜µì…˜)
        guard let arView = arView else { return }
        
        let extent = anchor.extent
        let mesh = MeshResource.generatePlane(
            width: extent.x,
            depth: extent.z
        )
        
        let material = SimpleMaterial(
            color: UIColor.green.withAlphaComponent(0.1),
            isMetallic: false
        )
        
        let planeEntity = ModelEntity(mesh: mesh, materials: [material])
        
        let anchorEntity = AnchorEntity(anchor: anchor)
        anchorEntity.addChild(planeEntity)
        
        arView.scene.addAnchor(anchorEntity)
    }
}

// MARK: - Supporting Types

struct CookingStep: Identifiable {
    let id = UUID()
    let title: String
    let description: String
    let duration: TimeInterval?
    let type: StepType
    let animationType: AnimationType?
    let tips: [String]
    let warningPoints: [String]
    
    enum StepType {
        case preparation
        case cutting
        case mixing
        case cooking
        case plating
        case serving
    }
}

enum AnimationType {
    case rotate
    case bounce
    case pulse
}

struct StepOverlay: View {
    let step: CookingStep
    let onNext: () -> Void
    
    var body: some View {
        VStack(alignment: .leading, spacing: 12) {
            HStack {
                Text(step.title)
                    .font(.headline)
                    .foregroundColor(.white)
                
                Spacer()
                
                if let duration = step.duration {
                    Label("\(Int(duration))ë¶„", systemImage: "timer")
                        .font(.caption)
                        .foregroundColor(.white)
                }
            }
            
            Text(step.description)
                .font(.subheadline)
                .foregroundColor(.white.opacity(0.9))
            
            if !step.tips.isEmpty {
                VStack(alignment: .leading, spacing: 4) {
                    Label("íŒ", systemImage: "lightbulb")
                        .font(.caption)
                        .foregroundColor(.yellow)
                    
                    ForEach(step.tips, id: \.self) { tip in
                        Text("â€¢ \(tip)")
                            .font(.caption)
                            .foregroundColor(.white.opacity(0.8))
                    }
                }
            }
            
            Button {
                onNext()
            } label: {
                Label("ë‹¤ìŒ ë‹¨ê³„", systemImage: "arrow.right")
                    .frame(maxWidth: .infinity)
                    .padding()
                    .background(Color.blue)
                    .foregroundColor(.white)
                    .cornerRadius(10)
            }
        }
        .padding()
        .background(Color.black.opacity(0.8))
        .cornerRadius(16)
    }
}
```

---

## ğŸ¯ ì—¬ê¸°ì„œ ë°°ìš´ ê²ƒ

### 1. **ì¹´ë©”ë¼ í†µí•©**
- AVFoundation í™œìš©
- ì‹¤ì‹œê°„ í”„ë ˆì„ ì²˜ë¦¬
- í¬ì»¤ìŠ¤/ë…¸ì¶œ ì œì–´
- ë©€í‹° ì¹´ë©”ë¼ ì§€ì›

### 2. **Vision í”„ë ˆì„ì›Œí¬**
- ê°ì²´ ì¸ì‹
- ë°”ì½”ë“œ ìŠ¤ìº”
- í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ì‹¤ì‹œê°„ ì¶”ì 

### 3. **ARKit êµ¬í˜„**
- 3D ì½˜í…ì¸  ë°°ì¹˜
- í‰ë©´ ê°ì§€
- ì œìŠ¤ì²˜ ì¸í„°ë™ì…˜
- ì• ë‹ˆë©”ì´ì…˜

### 4. **ì‹¤ì‹œê°„ ì²˜ë¦¬**
- í”„ë ˆì„ ìµœì í™”
- ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬
- ë©”ëª¨ë¦¬ ê´€ë¦¬
- ì„±ëŠ¥ íŠœë‹

---

## ğŸ‰ ì„±ê³µ í™•ì¸

**ì¹´ë©”ë¼ ê¸°ëŠ¥ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ì‹¤ì‹œê°„ìœ¼ë¡œ ì¬ë£Œ ì¸ì‹
- [ ] ë°”ì½”ë“œë¡œ ì œí’ˆ ì •ë³´ ì¡°íšŒ
- [ ] ARë¡œ ìš”ë¦¬ ë‹¨ê³„ ì•ˆë‚´
- [ ] ì˜ì–‘ ì •ë³´ ìë™ ì¶”ì¶œ
- [ ] ìŒì‹ ë³¼ë¥¨ ì¸¡ì • ê°€ëŠ¥

---

**ì™„ë²½í•©ë‹ˆë‹¤! ì¹´ë©”ë¼ì™€ ARì„ í™œìš©í•œ í˜ì‹ ì ì¸ ìš”ë¦¬ ê²½í—˜ì´ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.**