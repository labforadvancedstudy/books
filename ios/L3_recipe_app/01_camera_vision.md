# L3-01: Ïπ¥Î©îÎùºÏôÄ Vision ÌîÑÎ†àÏûÑÏõåÌÅ¨
## Ïã§ÏãúÍ∞Ñ Ïû¨Î£å Ïù∏Ïãù, AR ÏöîÎ¶¨ Í∞ÄÏù¥Îìú, ÏòÅÏñë Î∂ÑÏÑù

---

> **"The camera is becoming the keyboard of the future."**

Ïπ¥Î©îÎùºÏôÄ AIÎ•º Í≤∞Ìï©Ìï¥ ÏöîÎ¶¨Î•º ÌòÅÏã†Ï†ÅÏúºÎ°ú Î≥ÄÌôîÏãúÌÇµÎãàÎã§.

---

## üéØ Î™©Ìëú

**ÏôÑÏÑ± ÌõÑ Í≤∞Í≥ºÎ¨º**:
- Ïã§ÏãúÍ∞Ñ Ïû¨Î£å Ïä§Ï∫î
- AR Î†àÏãúÌîº Ïò§Î≤ÑÎ†àÏù¥  
- ÏùåÏãù Î≥ºÎ•® Ï∏°Ï†ï
- ÏπºÎ°úÎ¶¨ Ï∂îÏ†ï
- Î∞îÏΩîÎìú/QR Ïä§Ï∫î

---

## üì∏ Ïπ¥Î©îÎùº UI Íµ¨ÌòÑ

### 1Îã®Í≥Ñ: Ïä§Ï∫î Ïπ¥Î©îÎùº Î∑∞

**Features/Camera/ScannerCameraView.swift**:

```swift
import SwiftUI
import AVFoundation
import Vision
import ARKit

struct ScannerCameraView: View {
    @StateObject private var camera = CameraViewModel()
    @StateObject private var recognizer = IngredientRecognizer()
    @State private var showingResults = false
    @State private var capturedImage: UIImage?
    @State private var scanMode: ScanMode = .ingredient
    @State private var showingARMode = false
    
    enum ScanMode: String, CaseIterable {
        case ingredient = "Ïû¨Î£å"
        case barcode = "Î∞îÏΩîÎìú"
        case nutrition = "ÏòÅÏñë"
        case volume = "Ïö©Îüâ"
        
        var icon: String {
            switch self {
            case .ingredient: return "leaf"
            case .barcode: return "barcode"
            case .nutrition: return "chart.pie"
            case .volume: return "cube"
            }
        }
    }
    
    var body: some View {
        ZStack {
            // Ïπ¥Î©îÎùº ÌîÑÎ¶¨Î∑∞
            CameraPreviewView(session: camera.session)
                .ignoresSafeArea()
                .overlay(
                    ScanningOverlay(
                        detectedObjects: camera.detectedObjects,
                        scanMode: scanMode
                    )
                )
                .gesture(
                    // ÌÉ≠ÏúºÎ°ú Ìè¨Ïª§Ïä§
                    DragGesture(minimumDistance: 0)
                        .onEnded { value in
                            camera.focus(at: value.location)
                        }
                )
                .gesture(
                    // ÌïÄÏπòÎ°ú Ï§å
                    MagnificationGesture()
                        .onChanged { value in
                            camera.zoom(scale: value)
                        }
                )
            
            // UI Ïò§Î≤ÑÎ†àÏù¥
            VStack {
                // ÏÉÅÎã® Ïª®Ìä∏Î°§
                HStack {
                    Button {
                        camera.toggleFlash()
                    } label: {
                        Image(systemName: camera.isFlashOn ? "bolt.fill" : "bolt.slash")
                            .font(.title2)
                            .foregroundColor(.white)
                            .frame(width: 44, height: 44)
                            .background(Color.black.opacity(0.5))
                            .clipShape(Circle())
                    }
                    
                    Spacer()
                    
                    // Î™®Îìú ÏÑ†ÌÉù
                    Picker("Ïä§Ï∫î Î™®Îìú", selection: $scanMode) {
                        ForEach(ScanMode.allCases, id: \.self) { mode in
                            Label(mode.rawValue, systemImage: mode.icon)
                                .tag(mode)
                        }
                    }
                    .pickerStyle(SegmentedPickerStyle())
                    .frame(width: 200)
                    .background(Color.black.opacity(0.5))
                    .cornerRadius(8)
                    
                    Spacer()
                    
                    Button {
                        showingARMode.toggle()
                    } label: {
                        Image(systemName: "arkit")
                            .font(.title2)
                            .foregroundColor(.white)
                            .frame(width: 44, height: 44)
                            .background(Color.black.opacity(0.5))
                            .clipShape(Circle())
                    }
                }
                .padding()
                
                Spacer()
                
                // Ïã§ÏãúÍ∞Ñ Ïù∏Ïãù Í≤∞Í≥º
                if !camera.recognizedItems.isEmpty {
                    RecognitionResultsBar(items: camera.recognizedItems)
                        .padding(.horizontal)
                }
                
                // ÌïòÎã® Ïª®Ìä∏Î°§
                HStack(spacing: 40) {
                    // Í∞§Îü¨Î¶¨
                    Button {
                        camera.openPhotoLibrary()
                    } label: {
                        if let lastPhoto = camera.lastPhoto {
                            Image(uiImage: lastPhoto)
                                .resizable()
                                .scaledToFill()
                                .frame(width: 50, height: 50)
                                .clipShape(RoundedRectangle(cornerRadius: 8))
                        } else {
                            Image(systemName: "photo")
                                .font(.title2)
                                .foregroundColor(.white)
                                .frame(width: 50, height: 50)
                                .background(Color.black.opacity(0.5))
                                .clipShape(RoundedRectangle(cornerRadius: 8))
                        }
                    }
                    
                    // Ï∫°Ï≤ò Î≤ÑÌäº
                    Button {
                        capturePhoto()
                    } label: {
                        ZStack {
                            Circle()
                                .fill(Color.white)
                                .frame(width: 70, height: 70)
                            
                            Circle()
                                .stroke(Color.white, lineWidth: 3)
                                .frame(width: 80, height: 80)
                        }
                    }
                    
                    // Ïπ¥Î©îÎùº Ï†ÑÌôò
                    Button {
                        camera.switchCamera()
                    } label: {
                        Image(systemName: "camera.rotate")
                            .font(.title2)
                            .foregroundColor(.white)
                            .frame(width: 50, height: 50)
                            .background(Color.black.opacity(0.5))
                            .clipShape(RoundedRectangle(cornerRadius: 8))
                    }
                }
                .padding(.bottom, 30)
            }
        }
        .sheet(isPresented: $showingResults) {
            if let image = capturedImage {
                ScanResultsView(
                    image: image,
                    scanMode: scanMode,
                    recognizedItems: camera.recognizedItems
                )
            }
        }
        .fullScreenCover(isPresented: $showingARMode) {
            ARCookingGuideView()
        }
        .onAppear {
            camera.startSession()
        }
        .onDisappear {
            camera.stopSession()
        }
    }
    
    private func capturePhoto() {
        camera.capturePhoto { image in
            self.capturedImage = image
            
            Task {
                // Î™®ÎìúÎ≥Ñ Ï≤òÎ¶¨
                switch scanMode {
                case .ingredient:
                    await processIngredientScan(image)
                case .barcode:
                    await processBarcodeaScan(image)
                case .nutrition:
                    await processNutritionScan(image)
                case .volume:
                    await processVolumeScan(image)
                }
                
                showingResults = true
            }
        }
        
        // ÌñÖÌã± ÌîºÎìúÎ∞±
        let impactFeedback = UIImpactFeedbackGenerator(style: .medium)
        impactFeedback.impactOccurred()
    }
    
    private func processIngredientScan(_ image: UIImage) async {
        do {
            let ingredients = try await recognizer.recognizeIngredients(from: image)
            await MainActor.run {
                camera.recognizedItems = ingredients.map { RecognizedItem(from: $0) }
            }
        } catch {
            print("Ïû¨Î£å Ïù∏Ïãù Ïã§Ìå®: \(error)")
        }
    }
}

// MARK: - Camera Preview

struct CameraPreviewView: UIViewRepresentable {
    let session: AVCaptureSession
    
    func makeUIView(context: Context) -> UIView {
        let view = UIView()
        
        let previewLayer = AVCaptureVideoPreviewLayer(session: session)
        previewLayer.videoGravity = .resizeAspectFill
        view.layer.addSublayer(previewLayer)
        
        DispatchQueue.main.async {
            previewLayer.frame = view.bounds
        }
        
        return view
    }
    
    func updateUIView(_ uiView: UIView, context: Context) {
        if let previewLayer = uiView.layer.sublayers?.first as? AVCaptureVideoPreviewLayer {
            DispatchQueue.main.async {
                previewLayer.frame = uiView.bounds
            }
        }
    }
}

// MARK: - Scanning Overlay

struct ScanningOverlay: View {
    let detectedObjects: [DetectedObject]
    let scanMode: ScannerCameraView.ScanMode
    @State private var animationPhase = 0.0
    
    var body: some View {
        ZStack {
            // Ïä§Ï∫î Í∞ÄÏù¥Îìú
            if scanMode == .ingredient || scanMode == .nutrition {
                RoundedRectangle(cornerRadius: 20)
                    .stroke(
                        LinearGradient(
                            colors: [.blue, .purple],
                            startPoint: .topLeading,
                            endPoint: .bottomTrailing
                        ),
                        lineWidth: 3
                    )
                    .frame(width: 300, height: 300)
                    .opacity(0.8)
                    .scaleEffect(1.0 + sin(animationPhase) * 0.05)
                    .animation(
                        Animation.easeInOut(duration: 2)
                            .repeatForever(autoreverses: true),
                        value: animationPhase
                    )
            }
            
            // Í∞êÏßÄÎêú Í∞ùÏ≤¥ ÌëúÏãú
            ForEach(detectedObjects) { object in
                ObjectBoundingBox(object: object)
            }
            
            // Ïä§Ï∫î Î™®Îìú ÌëúÏãú
            VStack {
                Spacer()
                
                HStack {
                    Image(systemName: scanMode.icon)
                    Text("\(scanMode.rawValue) Ïä§Ï∫î Ï§ë...")
                }
                .padding()
                .background(Color.black.opacity(0.7))
                .foregroundColor(.white)
                .cornerRadius(20)
                .padding(.bottom, 100)
            }
        }
        .onAppear {
            animationPhase = .pi * 2
        }
    }
}

struct ObjectBoundingBox: View {
    let object: DetectedObject
    
    var body: some View {
        GeometryReader { geometry in
            let rect = VNImageRectForNormalizedRect(
                object.boundingBox,
                Int(geometry.size.width),
                Int(geometry.size.height)
            )
            
            ZStack(alignment: .topLeading) {
                Rectangle()
                    .stroke(object.color, lineWidth: 2)
                    .frame(width: rect.width, height: rect.height)
                    .position(
                        x: rect.midX,
                        y: geometry.size.height - rect.midY
                    )
                
                // ÎùºÎ≤®
                Text(object.label)
                    .font(.caption)
                    .padding(4)
                    .background(object.color)
                    .foregroundColor(.white)
                    .cornerRadius(4)
                    .position(
                        x: rect.minX,
                        y: geometry.size.height - rect.maxY
                    )
                
                // Ïã†Î¢∞ÎèÑ
                if object.confidence > 0 {
                    Text("\(Int(object.confidence * 100))%")
                        .font(.caption2)
                        .padding(2)
                        .background(Color.black.opacity(0.7))
                        .foregroundColor(.white)
                        .cornerRadius(4)
                        .position(
                            x: rect.maxX,
                            y: geometry.size.height - rect.maxY
                        )
                }
            }
        }
    }
}
```

### 2Îã®Í≥Ñ: Ïπ¥Î©îÎùº Î∑∞Î™®Îç∏

**Features/Camera/CameraViewModel.swift**:

```swift
import AVFoundation
import Vision
import Photos
import Combine
import UIKit

@MainActor
class CameraViewModel: NSObject, ObservableObject {
    @Published var session = AVCaptureSession()
    @Published var isFlashOn = false
    @Published var zoomFactor: CGFloat = 1.0
    @Published var recognizedItems: [RecognizedItem] = []
    @Published var detectedObjects: [DetectedObject] = []
    @Published var lastPhoto: UIImage?
    @Published var isProcessing = false
    
    private var photoOutput = AVCapturePhotoOutput()
    private var videoOutput = AVCaptureVideoDataOutput()
    private var currentDevice: AVCaptureDevice?
    private var captureCompletionHandler: ((UIImage) -> Void)?
    
    // Vision
    private var requests = [VNRequest]()
    private let visionQueue = DispatchQueue(label: "com.app.vision", qos: .userInitiated)
    
    override init() {
        super.init()
        setupCamera()
        setupVision()
        loadLastPhoto()
    }
    
    private func setupCamera() {
        session.beginConfiguration()
        session.sessionPreset = .photo
        
        // Ïπ¥Î©îÎùº Ï∂îÍ∞Ä
        guard let camera = AVCaptureDevice.default(
            .builtInWideAngleCamera,
            for: .video,
            position: .back
        ) else { return }
        
        currentDevice = camera
        
        do {
            let input = try AVCaptureDeviceInput(device: camera)
            if session.canAddInput(input) {
                session.addInput(input)
            }
            
            // ÏÇ¨ÏßÑ Ï∂úÎ†•
            if session.canAddOutput(photoOutput) {
                session.addOutput(photoOutput)
                photoOutput.isHighResolutionCaptureEnabled = true
                photoOutput.maxPhotoQualityPrioritization = .quality
            }
            
            // ÎπÑÎîîÏò§ Ï∂úÎ†• (Ïã§ÏãúÍ∞Ñ Î∂ÑÏÑùÏö©)
            videoOutput.setSampleBufferDelegate(self, queue: visionQueue)
            if session.canAddOutput(videoOutput) {
                session.addOutput(videoOutput)
            }
            
        } catch {
            print("Ïπ¥Î©îÎùº ÏÑ§Ï†ï Ïã§Ìå®: \(error)")
        }
        
        session.commitConfiguration()
    }
    
    private func setupVision() {
        // Í∞ùÏ≤¥ Ïù∏Ïãù
        guard let model = try? VNCoreMLModel(for: YOLOv3().model) else { return }
        
        let objectDetection = VNCoreMLRequest(model: model) { [weak self] request, error in
            self?.processDetections(request.results)
        }
        objectDetection.imageCropAndScaleOption = .scaleFill
        
        // Î∞îÏΩîÎìú Ïù∏Ïãù
        let barcodeRequest = VNDetectBarcodesRequest { [weak self] request, error in
            self?.processBarcodes(request.results)
        }
        
        // ÌÖçÏä§Ìä∏ Ïù∏Ïãù
        let textRequest = VNRecognizeTextRequest { [weak self] request, error in
            self?.processText(request.results)
        }
        textRequest.recognitionLevel = .accurate
        textRequest.recognitionLanguages = ["ko-KR", "en-US"]
        textRequest.usesLanguageCorrection = true
        
        requests = [objectDetection, barcodeRequest, textRequest]
    }
    
    func startSession() {
        if !session.isRunning {
            DispatchQueue.global(qos: .background).async { [weak self] in
                self?.session.startRunning()
            }
        }
    }
    
    func stopSession() {
        if session.isRunning {
            DispatchQueue.global(qos: .background).async { [weak self] in
                self?.session.stopRunning()
            }
        }
    }
    
    func capturePhoto(completion: @escaping (UIImage) -> Void) {
        captureCompletionHandler = completion
        
        let settings = AVCapturePhotoSettings()
        settings.flashMode = isFlashOn ? .on : .off
        
        // HEIF ÌòïÏãù ÏÇ¨Ïö© (Îçî ÎÇòÏùÄ ÏïïÏ∂ï)
        if photoOutput.availablePhotoCodecTypes.contains(.hevc) {
            settings.photoCodecType = .hevc
        }
        
        // ÍπäÏù¥ Îç∞Ïù¥ÌÑ∞ Ï∫°Ï≤ò
        if photoOutput.isDepthDataDeliverySupported {
            settings.isDepthDataDeliveryEnabled = true
        }
        
        // Ìè¨Ìä∏Î†àÏù¥Ìä∏ Ìö®Í≥º
        if photoOutput.isPortraitEffectsMatteDeliverySupported {
            settings.isPortraitEffectsMatteDeliveryEnabled = true
        }
        
        photoOutput.capturePhoto(with: settings, delegate: self)
    }
    
    func toggleFlash() {
        isFlashOn.toggle()
    }
    
    func switchCamera() {
        session.beginConfiguration()
        
        // ÌòÑÏû¨ ÏûÖÎ†• Ï†úÍ±∞
        session.inputs.forEach { session.removeInput($0) }
        
        // ÏÉà Ïπ¥Î©îÎùº ÏÑ†ÌÉù
        let position: AVCaptureDevice.Position = currentDevice?.position == .back ? .front : .back
        
        guard let newCamera = AVCaptureDevice.default(
            .builtInWideAngleCamera,
            for: .video,
            position: position
        ) else { return }
        
        do {
            let input = try AVCaptureDeviceInput(device: newCamera)
            if session.canAddInput(input) {
                session.addInput(input)
                currentDevice = newCamera
            }
        } catch {
            print("Ïπ¥Î©îÎùº Ï†ÑÌôò Ïã§Ìå®: \(error)")
        }
        
        session.commitConfiguration()
    }
    
    func zoom(scale: CGFloat) {
        guard let device = currentDevice else { return }
        
        do {
            try device.lockForConfiguration()
            
            let maxZoom = min(device.activeFormat.videoMaxZoomFactor, 10.0)
            let newZoom = max(1.0, min(scale * zoomFactor, maxZoom))
            
            device.videoZoomFactor = newZoom
            zoomFactor = newZoom
            
            device.unlockForConfiguration()
        } catch {
            print("Ï§å ÏÑ§Ï†ï Ïã§Ìå®: \(error)")
        }
    }
    
    func focus(at point: CGPoint) {
        guard let device = currentDevice else { return }
        
        do {
            try device.lockForConfiguration()
            
            if device.isFocusPointOfInterestSupported {
                device.focusPointOfInterest = point
                device.focusMode = .autoFocus
            }
            
            if device.isExposurePointOfInterestSupported {
                device.exposurePointOfInterest = point
                device.exposureMode = .autoExpose
            }
            
            device.unlockForConfiguration()
            
            // Ìè¨Ïª§Ïä§ Ïï†ÎãàÎ©îÏù¥ÏÖò ÌîºÎìúÎ∞±
            let generator = UIImpactFeedbackGenerator(style: .light)
            generator.impactOccurred()
        } catch {
            print("Ìè¨Ïª§Ïä§ ÏÑ§Ï†ï Ïã§Ìå®: \(error)")
        }
    }
    
    func openPhotoLibrary() {
        // Photo picker Ïó¥Í∏∞
        // SwiftUIÏùò PhotosPicker ÏÇ¨Ïö©
    }
    
    private func loadLastPhoto() {
        let fetchOptions = PHFetchOptions()
        fetchOptions.sortDescriptors = [NSSortDescriptor(key: "creationDate", ascending: false)]
        fetchOptions.fetchLimit = 1
        
        let fetchResult = PHAsset.fetchAssets(with: .image, options: fetchOptions)
        
        guard let asset = fetchResult.firstObject else { return }
        
        let manager = PHImageManager.default()
        let options = PHImageRequestOptions()
        options.isSynchronous = false
        options.deliveryMode = .highQualityFormat
        
        manager.requestImage(
            for: asset,
            targetSize: CGSize(width: 100, height: 100),
            contentMode: .aspectFill,
            options: options
        ) { [weak self] image, _ in
            DispatchQueue.main.async {
                self?.lastPhoto = image
            }
        }
    }
}

// MARK: - AVCapturePhotoCaptureDelegate

extension CameraViewModel: AVCapturePhotoCaptureDelegate {
    func photoOutput(_ output: AVCapturePhotoOutput, didFinishProcessingPhoto photo: AVCapturePhoto, error: Error?) {
        guard error == nil,
              let imageData = photo.fileDataRepresentation(),
              let image = UIImage(data: imageData) else { return }
        
        // Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•
        UIImageWriteToSavedPhotosAlbum(image, nil, nil, nil)
        
        DispatchQueue.main.async {
            self.lastPhoto = image
            self.captureCompletionHandler?(image)
            self.captureCompletionHandler = nil
        }
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate

extension CameraViewModel: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard !isProcessing,
              let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        isProcessing = true
        
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        
        do {
            try handler.perform(requests)
        } catch {
            print("Vision Ï≤òÎ¶¨ Ïã§Ìå®: \(error)")
        }
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            self.isProcessing = false
        }
    }
    
    private func processDetections(_ results: [Any]?) {
        guard let results = results as? [VNRecognizedObjectObservation] else { return }
        
        let objects = results.compactMap { observation -> DetectedObject? in
            guard observation.confidence > 0.5 else { return nil }
            
            return DetectedObject(
                id: UUID(),
                label: observation.labels.first?.identifier ?? "Unknown",
                confidence: observation.confidence,
                boundingBox: observation.boundingBox,
                color: .blue
            )
        }
        
        DispatchQueue.main.async {
            self.detectedObjects = objects
        }
    }
    
    private func processBarcodes(_ results: [Any]?) {
        guard let results = results as? [VNBarcodeObservation] else { return }
        
        for barcode in results {
            if let payload = barcode.payloadStringValue {
                print("Î∞îÏΩîÎìú Í∞êÏßÄ: \(payload)")
                
                // Î∞îÏΩîÎìúÎ°ú Ï†úÌíà Ï°∞Ìöå
                Task {
                    await lookupProduct(barcode: payload)
                }
            }
        }
    }
    
    private func processText(_ results: [Any]?) {
        guard let results = results as? [VNRecognizedTextObservation] else { return }
        
        for observation in results {
            guard let text = observation.topCandidates(1).first?.string else { continue }
            
            // ÏòÅÏñë Ï†ïÎ≥¥ Ï∂îÏ∂ú
            if text.contains("ÏπºÎ°úÎ¶¨") || text.contains("kcal") {
                extractNutritionInfo(from: text)
            }
        }
    }
}

// MARK: - Supporting Types

struct DetectedObject: Identifiable {
    let id: UUID
    let label: String
    let confidence: Float
    let boundingBox: CGRect
    let color: Color
}

struct RecognizedItem: Identifiable {
    let id: UUID
    let name: String
    let category: String
    let confidence: Double
    let nutrition: NutritionInfo?
}
```

### 3Îã®Í≥Ñ: AR ÏöîÎ¶¨ Í∞ÄÏù¥Îìú

**Features/Camera/ARCookingGuideView.swift**:

```swift
import SwiftUI
import RealityKit
import ARKit
import Combine

struct ARCookingGuideView: View {
    @StateObject private var arViewModel = ARCookingViewModel()
    @State private var currentStep = 0
    @State private var showingStepDetail = false
    @Environment(\.dismiss) private var dismiss
    
    var body: some View {
        ZStack {
            // AR View
            ARViewContainer(viewModel: arViewModel)
                .ignoresSafeArea()
            
            // UI Ïò§Î≤ÑÎ†àÏù¥
            VStack {
                // Ìó§Îçî
                HStack {
                    Button {
                        dismiss()
                    } label: {
                        Image(systemName: "xmark.circle.fill")
                            .font(.title)
                            .foregroundColor(.white)
                            .background(Color.black.opacity(0.5))
                            .clipShape(Circle())
                    }
                    
                    Spacer()
                    
                    Text(arViewModel.recipe?.name ?? "AR ÏöîÎ¶¨ Í∞ÄÏù¥Îìú")
                        .font(.headline)
                        .padding(.horizontal, 16)
                        .padding(.vertical, 8)
                        .background(Color.black.opacity(0.7))
                        .foregroundColor(.white)
                        .cornerRadius(20)
                    
                    Spacer()
                    
                    Button {
                        arViewModel.toggleARMode()
                    } label: {
                        Image(systemName: arViewModel.isAREnabled ? "arkit" : "arkit.slash")
                            .font(.title)
                            .foregroundColor(.white)
                            .background(Color.black.opacity(0.5))
                            .clipShape(Circle())
                    }
                }
                .padding()
                
                Spacer()
                
                // ÌòÑÏû¨ Îã®Í≥Ñ ÌëúÏãú
                if let step = arViewModel.currentStep {
                    StepOverlay(step: step, onNext: {
                        arViewModel.nextStep()
                    })
                    .padding()
                }
                
                // AR Ïù∏ÎîîÏºÄÏù¥ÌÑ∞
                if arViewModel.isTracking {
                    HStack {
                        Circle()
                            .fill(Color.green)
                            .frame(width: 10, height: 10)
                        Text("ÌëúÎ©¥ Ïù∏ÏãùÎê®")
                            .font(.caption)
                            .foregroundColor(.white)
                    }
                    .padding(8)
                    .background(Color.black.opacity(0.7))
                    .cornerRadius(20)
                    .padding(.bottom)
                }
            }
        }
        .onAppear {
            arViewModel.startAR()
        }
        .onDisappear {
            arViewModel.stopAR()
        }
    }
}

struct ARViewContainer: UIViewRepresentable {
    let viewModel: ARCookingViewModel
    
    func makeUIView(context: Context) -> ARView {
        let arView = ARView(frame: .zero)
        
        // AR ÏÑ§Ï†ï
        let config = ARWorldTrackingConfiguration()
        config.planeDetection = [.horizontal]
        config.environmentTexturing = .automatic
        
        arView.session.run(config)
        arView.session.delegate = context.coordinator
        
        // Ï†úÏä§Ï≤ò Ï∂îÍ∞Ä
        let tapGesture = UITapGestureRecognizer(
            target: context.coordinator,
            action: #selector(Coordinator.handleTap)
        )
        arView.addGestureRecognizer(tapGesture)
        
        viewModel.arView = arView
        
        return arView
    }
    
    func updateUIView(_ uiView: ARView, context: Context) {
        // ÏóÖÎç∞Ïù¥Ìä∏ ÌïÑÏöîÏãú Ï≤òÎ¶¨
    }
    
    func makeCoordinator() -> Coordinator {
        Coordinator(viewModel: viewModel)
    }
    
    class Coordinator: NSObject, ARSessionDelegate {
        let viewModel: ARCookingViewModel
        
        init(viewModel: ARCookingViewModel) {
            self.viewModel = viewModel
        }
        
        @objc func handleTap(gesture: UITapGestureRecognizer) {
            guard let arView = viewModel.arView else { return }
            
            let location = gesture.location(in: arView)
            
            // Î†àÏù¥Ï∫êÏä§Ìä∏Î°ú ÌëúÎ©¥ Ï∞æÍ∏∞
            let results = arView.raycast(
                from: location,
                allowing: .existingPlaneGeometry,
                alignment: .horizontal
            )
            
            if let firstResult = results.first {
                viewModel.placeContent(at: firstResult)
            }
        }
        
        func session(_ session: ARSession, didUpdate frame: ARFrame) {
            // Ìä∏ÎûòÌÇπ ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            DispatchQueue.main.async {
                self.viewModel.updateTrackingState(frame.camera.trackingState)
            }
        }
        
        func session(_ session: ARSession, didAdd anchors: [ARAnchor]) {
            for anchor in anchors {
                if let planeAnchor = anchor as? ARPlaneAnchor {
                    viewModel.addPlane(planeAnchor)
                }
            }
        }
    }
}

@MainActor
class ARCookingViewModel: ObservableObject {
    @Published var recipe: Recipe?
    @Published var currentStep: CookingStep?
    @Published var isAREnabled = true
    @Published var isTracking = false
    
    weak var arView: ARView?
    private var anchorEntities: [AnchorEntity] = []
    private var stepEntities: [ModelEntity] = []
    
    func startAR() {
        loadRecipe()
        setupARContent()
    }
    
    func stopAR() {
        arView?.session.pause()
        clearARContent()
    }
    
    func toggleARMode() {
        isAREnabled.toggle()
        
        if isAREnabled {
            showARContent()
        } else {
            hideARContent()
        }
    }
    
    func nextStep() {
        guard let recipe = recipe,
              let currentIndex = recipe.steps.firstIndex(where: { $0.id == currentStep?.id }) else { return }
        
        if currentIndex < recipe.steps.count - 1 {
            currentStep = recipe.steps[currentIndex + 1]
            updateARContent()
        }
    }
    
    func placeContent(at result: ARRaycastResult) {
        guard let arView = arView else { return }
        
        // ÏïµÏª§ ÏÉùÏÑ±
        let anchor = AnchorEntity(world: result.worldTransform)
        arView.scene.addAnchor(anchor)
        
        // 3D ÏΩòÌÖêÏ∏† Ï∂îÍ∞Ä
        if let step = currentStep {
            let content = create3DContent(for: step)
            anchor.addChild(content)
            anchorEntities.append(anchor)
        }
        
        // ÌñÖÌã± ÌîºÎìúÎ∞±
        let generator = UIImpactFeedbackGenerator(style: .medium)
        generator.impactOccurred()
    }
    
    private func create3DContent(for step: CookingStep) -> ModelEntity {
        // Îã®Í≥ÑÎ≥Ñ 3D Î™®Îç∏ ÏÉùÏÑ±
        let mesh: MeshResource
        let material = SimpleMaterial(color: .systemBlue, isMetallic: true)
        
        switch step.type {
        case .cutting:
            // ÏπºÍ≥º ÎèÑÎßà Î™®Îç∏
            mesh = MeshResource.generateBox(size: [0.2, 0.01, 0.3])
            
        case .mixing:
            // Î≥ºÍ≥º Ïä§Ìëº Î™®Îç∏
            mesh = MeshResource.generateSphere(radius: 0.15)
            
        case .cooking:
            // Ìå¨ Î™®Îç∏
            mesh = MeshResource.generateCylinder(height: 0.05, radius: 0.2)
            
        case .plating:
            // Ï†ëÏãú Î™®Îç∏
            mesh = MeshResource.generatePlane(width: 0.3, depth: 0.3)
            
        default:
            mesh = MeshResource.generateBox(size: 0.1)
        }
        
        let model = ModelEntity(mesh: mesh, materials: [material])
        
        // ÌÖçÏä§Ìä∏ ÎùºÎ≤® Ï∂îÍ∞Ä
        if let textMesh = try? MeshResource.generateText(
            step.title,
            extrusionDepth: 0.01,
            font: .systemFont(ofSize: 0.05)
        ) {
            let textEntity = ModelEntity(mesh: textMesh, materials: [material])
            textEntity.position = [0, 0.2, 0]
            model.addChild(textEntity)
        }
        
        // Ïï†ÎãàÎ©îÏù¥ÏÖò Ï∂îÍ∞Ä
        addAnimation(to: model, type: step.animationType)
        
        return model
    }
    
    private func addAnimation(to entity: ModelEntity, type: AnimationType?) {
        guard let type = type else { return }
        
        switch type {
        case .rotate:
            entity.transform.rotation = simd_quatf(angle: 0, axis: [0, 1, 0])
            
            var transform = entity.transform
            transform.rotation = simd_quatf(angle: .pi * 2, axis: [0, 1, 0])
            
            entity.move(
                to: transform,
                relativeTo: entity.parent,
                duration: 3,
                timingFunction: .linear
            )
            
        case .bounce:
            let originalPosition = entity.position
            
            var transform = entity.transform
            transform.translation.y += 0.1
            
            entity.move(
                to: transform,
                relativeTo: entity.parent,
                duration: 1,
                timingFunction: .easeInOut
            )
            
        case .pulse:
            let originalScale = entity.scale
            
            var transform = entity.transform
            transform.scale = originalScale * 1.2
            
            entity.move(
                to: transform,
                relativeTo: entity.parent,
                duration: 0.5,
                timingFunction: .easeInOut
            )
        }
    }
    
    func updateTrackingState(_ state: ARCamera.TrackingState) {
        switch state {
        case .normal:
            isTracking = true
        case .limited:
            isTracking = false
        case .notAvailable:
            isTracking = false
        }
    }
    
    func addPlane(_ anchor: ARPlaneAnchor) {
        // ÌèâÎ©¥ ÏãúÍ∞ÅÌôî (ÏòµÏÖò)
        guard let arView = arView else { return }
        
        let extent = anchor.extent
        let mesh = MeshResource.generatePlane(
            width: extent.x,
            depth: extent.z
        )
        
        let material = SimpleMaterial(
            color: UIColor.green.withAlphaComponent(0.1),
            isMetallic: false
        )
        
        let planeEntity = ModelEntity(mesh: mesh, materials: [material])
        
        let anchorEntity = AnchorEntity(anchor: anchor)
        anchorEntity.addChild(planeEntity)
        
        arView.scene.addAnchor(anchorEntity)
    }
}

// MARK: - Supporting Types

struct CookingStep: Identifiable {
    let id = UUID()
    let title: String
    let description: String
    let duration: TimeInterval?
    let type: StepType
    let animationType: AnimationType?
    let tips: [String]
    let warningPoints: [String]
    
    enum StepType {
        case preparation
        case cutting
        case mixing
        case cooking
        case plating
        case serving
    }
}

enum AnimationType {
    case rotate
    case bounce
    case pulse
}

struct StepOverlay: View {
    let step: CookingStep
    let onNext: () -> Void
    
    var body: some View {
        VStack(alignment: .leading, spacing: 12) {
            HStack {
                Text(step.title)
                    .font(.headline)
                    .foregroundColor(.white)
                
                Spacer()
                
                if let duration = step.duration {
                    Label("\(Int(duration))Î∂Ñ", systemImage: "timer")
                        .font(.caption)
                        .foregroundColor(.white)
                }
            }
            
            Text(step.description)
                .font(.subheadline)
                .foregroundColor(.white.opacity(0.9))
            
            if !step.tips.isEmpty {
                VStack(alignment: .leading, spacing: 4) {
                    Label("ÌåÅ", systemImage: "lightbulb")
                        .font(.caption)
                        .foregroundColor(.yellow)
                    
                    ForEach(step.tips, id: \.self) { tip in
                        Text("‚Ä¢ \(tip)")
                            .font(.caption)
                            .foregroundColor(.white.opacity(0.8))
                    }
                }
            }
            
            Button {
                onNext()
            } label: {
                Label("Îã§Ïùå Îã®Í≥Ñ", systemImage: "arrow.right")
                    .frame(maxWidth: .infinity)
                    .padding()
                    .background(Color.blue)
                    .foregroundColor(.white)
                    .cornerRadius(10)
            }
        }
        .padding()
        .background(Color.black.opacity(0.8))
        .cornerRadius(16)
    }
}
```

---

## üéØ Ïó¨Í∏∞ÏÑú Î∞∞Ïö¥ Í≤É

### 1. **Ïπ¥Î©îÎùº ÌÜµÌï©**
- AVFoundation ÌôúÏö©
- Ïã§ÏãúÍ∞Ñ ÌîÑÎ†àÏûÑ Ï≤òÎ¶¨
- Ìè¨Ïª§Ïä§/ÎÖ∏Ï∂ú Ï†úÏñ¥
- Î©ÄÌã∞ Ïπ¥Î©îÎùº ÏßÄÏõê

### 2. **Vision ÌîÑÎ†àÏûÑÏõåÌÅ¨**
- Í∞ùÏ≤¥ Ïù∏Ïãù
- Î∞îÏΩîÎìú Ïä§Ï∫î
- ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
- Ïã§ÏãúÍ∞Ñ Ï∂îÏ†Å

### 3. **ARKit Íµ¨ÌòÑ**
- 3D ÏΩòÌÖêÏ∏† Î∞∞Ïπò
- ÌèâÎ©¥ Í∞êÏßÄ
- Ï†úÏä§Ï≤ò Ïù∏ÌÑ∞ÎûôÏÖò
- Ïï†ÎãàÎ©îÏù¥ÏÖò

### 4. **Ïã§ÏãúÍ∞Ñ Ï≤òÎ¶¨**
- ÌîÑÎ†àÏûÑ ÏµúÏ†ÅÌôî
- Î∞±Í∑∏ÎùºÏö¥Îìú Ï≤òÎ¶¨
- Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨
- ÏÑ±Îä• ÌäúÎãù

---

## üéâ ÏÑ±Í≥µ ÌôïÏù∏

**Ïπ¥Î©îÎùº Í∏∞Îä• Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏**:
- [ ] Ïã§ÏãúÍ∞ÑÏúºÎ°ú Ïû¨Î£å Ïù∏Ïãù
- [ ] Î∞îÏΩîÎìúÎ°ú Ï†úÌíà Ï†ïÎ≥¥ Ï°∞Ìöå
- [ ] ARÎ°ú ÏöîÎ¶¨ Îã®Í≥Ñ ÏïàÎÇ¥
- [ ] ÏòÅÏñë Ï†ïÎ≥¥ ÏûêÎèô Ï∂îÏ∂ú
- [ ] ÏùåÏãù Î≥ºÎ•® Ï∏°Ï†ï Í∞ÄÎä•

---

**ÏôÑÎ≤ΩÌï©ÎãàÎã§! Ïπ¥Î©îÎùºÏôÄ ARÏùÑ ÌôúÏö©Ìïú ÌòÅÏã†Ï†ÅÏù∏ ÏöîÎ¶¨ Í≤ΩÌóòÏù¥ Íµ¨ÌòÑÎêòÏóàÏäµÎãàÎã§.**